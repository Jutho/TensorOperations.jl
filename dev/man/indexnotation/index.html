<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Index notation with macros · TensorOperations.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.svg" alt="TensorOperations.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">TensorOperations.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Manual</span><ul><li class="is-active"><a class="tocitem" href>Index notation with macros</a><ul class="internal"><li><a class="tocitem" href="#The-@tensor-macro"><span>The <code>@tensor</code> macro</span></a></li><li><a class="tocitem" href="#Contraction-order-specification-and-optimisation"><span>Contraction order specification and optimisation</span></a></li><li><a class="tocitem" href="#Dynamical-tensor-network-contractions-with-ncon-and-@ncon"><span>Dynamical tensor network contractions with <code>ncon</code> and <code>@ncon</code></span></a></li><li><a class="tocitem" href="#Index-compatibility-and-checks"><span>Index compatibility and checks</span></a></li><li><a class="tocitem" href="#Backends,-multithreading-and-GPUs"><span>Backends, multithreading and GPUs</span></a></li></ul></li><li><a class="tocitem" href="../functions/">Functions</a></li><li><a class="tocitem" href="../interface/">Interface</a></li><li><a class="tocitem" href="../implementation/">Implementation</a></li><li><a class="tocitem" href="../autodiff/">Automatic differentiation</a></li></ul></li><li><a class="tocitem" href="../../index/">Index</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Manual</a></li><li class="is-active"><a href>Index notation with macros</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Index notation with macros</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/Jutho/TensorOperations.jl/blob/master/docs/src/man/indexnotation.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Index-notation-with-macros"><a class="docs-heading-anchor" href="#Index-notation-with-macros">Index notation with macros</a><a id="Index-notation-with-macros-1"></a><a class="docs-heading-anchor-permalink" href="#Index-notation-with-macros" title="Permalink"></a></h1><p>The main export and main functionality of TensorOperations.jl is the <code>@tensor</code> macro</p><article class="docstring"><header><a class="docstring-binding" id="TensorOperations.@tensor" href="#TensorOperations.@tensor"><code>TensorOperations.@tensor</code></a> — <span class="docstring-category">Macro</span></header><section><div><pre><code class="language-julia hljs">@tensor(tensor_expr; kwargs...)
@tensor [kw_expr...] tensor_expr</code></pre><p>Specify one or more tensor operations using Einstein&#39;s index notation. Indices can be chosen to be arbitrary Julia variable names, or integers. When contracting several tensors together, this will be evaluated as pairwise contractions in left to right order, unless the so-called NCON style is used (positive integers for contracted indices and negative indices for open indices).</p><p>Additional keyword arguments may be passed to control the behavior of the parser:</p><ul><li><code>order</code>:    A list of contraction indices of the form <code>order=(...,)</code> which specify the order in which they will be contracted.</li><li><code>opt</code>:   Contraction order optimization, similar to <a href="man/@ref"><code>@tensoropt</code></a>. Can be either a boolean or an <code>OptExpr</code>.</li><li><code>contractcheck</code>:   Boolean flag to enable runtime check for contractibility of indices with clearer error messages.</li><li><code>costcheck</code>:   Adds runtime checks to ensure that the contraction order is optimal. Can be either <code>:warn</code> or <code>:cache</code>. The former will issues warnings when sub-optimal expressions are encountered, while the latter will cache the optimal contraction order for each tensor site and calling site.</li><li><code>backend</code>:    Inserts a backend call for the different tensor operations.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Jutho/TensorOperations.jl/blob/9975b3139399ad4c05a63e298433d5b099e15b5f/src/indexnotation/tensormacros.jl#L12-L34">source</a></section></article><p>The functionality and configurability of <code>@tensor</code> and some of its relatives is explained in detail on this page.</p><h2 id="The-@tensor-macro"><a class="docs-heading-anchor" href="#The-@tensor-macro">The <code>@tensor</code> macro</a><a id="The-@tensor-macro-1"></a><a class="docs-heading-anchor-permalink" href="#The-@tensor-macro" title="Permalink"></a></h2><p>The prefered way to specify (a sequence of) tensor operations is by using the <code>@tensor</code> macro, which accepts an <a href="https://en.wikipedia.org/wiki/Abstract_index_notation">index notation</a> format, a.k.a. <a href="https://en.wikipedia.org/wiki/Einstein_notation">Einstein notation</a> (and in particular, Einstein&#39;s summation convention).</p><p>This can most easily be explained using a simple example:</p><pre><code class="language-julia hljs">using TensorOperations
α = randn()
A = randn(5, 5, 5, 5, 5, 5)
B = randn(5, 5, 5)
C = randn(5, 5, 5)
D = zeros(5, 5, 5)
@tensor begin
    D[a, b, c] = A[a, e, f, c, f, g] * B[g, b, e] + α * C[c, a, b]
    E[a, b, c] := A[a, e, f, c, f, g] * B[g, b, e] + α * C[c, a, b]
end</code></pre><p>The first thing to note is the use of two different assignment operators within the body of the <code>@tensor</code> call. The regular assignment operator <code>=</code> stores the result of the tensor expression in the right hand side in an existing tensor <code>D</code>, whereas the alternative assignment operator <code>:=</code> results in a new tensor <code>E</code> with the correct properties to be created. However, the contents of <code>D</code> and <code>E</code> will be equal.</p><p>Following Einstein&#39;s summation convention, these contents are computed in a number of steps involving the three primitive tensor operators. In this particular example, the first step involves tracing/ contracting the 3rd and 5th index of array <code>A</code>. The resulting array will then be contracted with array <code>B</code> by contracting its 2nd index with the last index of <code>B</code> and its last index with the first index of <code>B</code>. The resulting array has three remaining indices, which correspond to the indices <code>a</code> and <code>c</code> of array <code>A</code> and index <code>b</code> of array <code>B</code> (in that order). To this, the array <code>C</code> (scaled with <code>α</code>) is added, where its first two indices will be permuted to fit with the order <code>a, c, b</code>. The result will then be stored in array <code>D</code> (or <code>E</code>), which requires a second permutation to bring the indices in the requested order <code>a, b, c</code>.</p><p>The index pattern is analyzed at compile time and expanded to a set of calls to the basic tensor operations, i.e. <a href="../functions/#TensorOperations.tensoradd!"><code>tensoradd!</code></a>, <a href="../functions/#TensorOperations.tensortrace!"><code>tensortrace!</code></a> and <a href="../functions/#TensorOperations.tensorcontract!"><code>tensorcontract!</code></a>. Temporaries are created where necessary, as these building blocks operate pairwise on the input tensors. The generated code can easily be inspected</p><pre><code class="language-julia hljs">using TensorOperations
@macroexpand @tensor E[a, b, c] := A[a, e, f, c, f, g] * B[g, b, e] + α * C[c, a, b]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">quote
    var&quot;##T_E#309&quot; = TensorOperations.promote_add(TensorOperations.promote_contract(TensorOperations.scalartype(A), TensorOperations.scalartype(B)), Base.promote_op(*, TensorOperations.scalartype(C), TensorOperations.scalartype(α * true)))
    var&quot;##T_##E_A#310#311&quot; = TensorOperations.scalartype(A)
    var&quot;##E_A#310&quot; = TensorOperations.tensoralloc_add(var&quot;##T_##E_A#310#311&quot;, ((1, 4), (2, 6)), A, :N, true)
    var&quot;##E_A#310&quot; = TensorOperations.tensortrace!(var&quot;##E_A#310&quot;, ((1, 4), (2, 6)), A, ((3,), (5,)), :N, true, false)
    var&quot;##T_E#312&quot; = Base.promote_op(*, TensorOperations.scalartype(one(var&quot;##T_E#309&quot;)), TensorOperations.promote_contract(TensorOperations.scalartype(var&quot;##E_A#310&quot;), TensorOperations.scalartype(B)))
    E = TensorOperations.tensoralloc_contract(var&quot;##T_E#312&quot;, ((1, 3, 2), ()), var&quot;##E_A#310&quot;, ((1, 2), (3, 4)), :N, B, ((3, 1), (2,)), :N, false)
    E = TensorOperations.tensorcontract!(E, ((1, 3, 2), ()), var&quot;##E_A#310&quot;, ((1, 2), (3, 4)), :N, B, ((3, 1), (2,)), :N, one(var&quot;##T_E#309&quot;), false)
    TensorOperations.tensorfree!(var&quot;##E_A#310&quot;)
    E
    E = TensorOperations.tensoradd!(E, ((2, 3, 1), ()), C, :N, α * true, true)
end</code></pre><p>The different functions in which this tensor expression is decomposed are discussed in more detail in the <a href="../implementation/#Implementation">Implementation</a> section of this manual.</p><p>In this example, the tensor indices were labeled with arbitrary letters; also longer names could have been used. In fact, any proper Julia variable name constitutes a valid label. Note though that these labels are never interpreted as existing Julia variables. Within the <code>@tensor</code> macro they are converted into symbols and then used as dummy names, whose only role is to distinguish the different indices. Their specific value bears no meaning. They also do not appear in the generated code as illustrated above. This implies, in particular, that the specific tensor operations defined by the code inside the <code>@tensor</code> environment are completely specified at compile time. Various remarks regarding the index notation are in order.</p><ol><li><p>TensorOperations.jl only supports strict Einstein summation convention. This implies  that there are two types of indices. Either an index label appears once in every term of  the right hand side, and it also appears on the left hand side. We refer to the  corresponding indices as <em>open or free</em>. Alternatively, an index label appears exactly  twice within a given term on the right hand side. The corresponding indices are referred  to as <em>closed or contracted</em>, i.e. the pair of indices takes equal values and are summed  over their (equal) range. This is known as a contraction, either an outer contraction  (between two indices of two different tensors) or an inner contraction (a.k.a. trace,  between two indices of a single tensor). More liberal use of the index notation, such as  simultaneous summutation over three or more indices, or a open index appearing  simultaneously in different tensor factors, are not supported by TensorOperations.jl</p></li><li><p>Aside from valid Julia identifiers, index labels can also be specified using literal  integer constants or using a combination of integers and symbols. Furthermore, it is  also allowed to use primes (i.e. Julia&#39;s <code>adjoint</code> operator) to denote different  indices, including using multiple subsequent primes. The following expression thus  computes the same result as the example above:</p><p><code>julia  @tensor D[å&#39;&#39;, ß, clap&#39;] = A[å&#39;&#39;, 1, -3, clap&#39;, -3, 2] * B[2, ß, 1] + α * C[c&#39;, å&#39;&#39;, ß]</code></p></li><li><p>If only integers are used for specifying index labels, this can be used to control the  pairwise contraction order, by using the well-known NCON convention, where &#39;open  indices&#39; (appearing) in the left hand side are labelled by negative integers <code>-1</code>, <code>-2</code>,  <code>-3</code>, whereas contraction indices are labelled with positive integers <code>1</code>, <code>2</code>, … Since  the index order of the left hand side is in that case clear from the right hand side  expression, the left hand side can be indexed with <code>[:]</code>, which is automatically  replaced with all negative integers appearing in the right hand side, in decreasing  order. The value of the labels for the contraction indices determines the pairwise  contraction order. If multiple tensors need to be contracted, a first temporary will be  created consisting of the contraction of the pair of tensors that share contraction  index <code>1</code>, then the pair of tensors that share contraction index <code>2</code> (if not contracted  away in the first pair) will be contracted, and so forth. The next subsection explains  contraction order in more detail and gives some useful examples, as the example above  only includes a single pair of tensors to be contracted.</p></li><li><p>Index expressions <code>[...]</code> are only interpreted as index notation on the highest level.  For example, if you want to mulitply two matrices which are stored in a list, you can  write</p><p><code>@tensor result[i,j] := list[1][i,k] * list[2][k,j]</code></p><p>However, if both are stored as a the slices of a 3-way array, you cannot write</p><p><code>@tensor result[i,j] := list[i,k,1] * list[k,j,2]</code></p><p>Rather, you should use</p><p><code>@tensor result[i,j] := list[:,:,1][i,k] * list[:,:,2][k,j]</code></p><p>or, if you want to avoid additional allocations</p><p><code>@tensor result[i,j] := view(list, :,:,1)[i,k] * view(list, :,:,2)[k,j]</code></p></li></ol><p>Note, finally, that the <code>@tensor</code> specifier can be put in front of a single tensor expression, or in front of a <code>begin ... end</code> block to group and evaluate different expressions at once. Within an <code>@tensor begin ... end</code> block, the <code>@notensor</code> macro can be used to annotate indexing expressions that need to be interpreted literally. The previous matrix multiplication example with matrices stored in a 3-way array could for example also be written as</p><pre><code class="language-julia hljs">@tensor begin
    @notensor A = list[:,:,1]
    @notensor B = list[:,:,2]
    result[i,j] = A[i,k] * B[k,j]
end</code></pre><h2 id="Contraction-order-specification-and-optimisation"><a class="docs-heading-anchor" href="#Contraction-order-specification-and-optimisation">Contraction order specification and optimisation</a><a id="Contraction-order-specification-and-optimisation-1"></a><a class="docs-heading-anchor-permalink" href="#Contraction-order-specification-and-optimisation" title="Permalink"></a></h2><p>A contraction of several (more than two) tensors, as in</p><pre><code class="language-julia hljs">@tensor D[a, d, j, i, g] := A[a, b, c, d, e] * B[b, e, f, g] * C[c, f, i, j]</code></pre><p>is known as a <em>tensor network</em> and is generically evaluated as a sequence of pairwise contractions. In the example above, this contraction is evaluated using Julia&#39;s default left to right order, i.e. as <code>(A[a, b, c, d, e] * B[b, e, f, g]) * C[c, f, i, j]</code>. There are however different strategies to modify this order.</p><ol><li><p>Explicit parenthesis can be used to group subnetworks within a tensor network that will  be evaluated first. Parentheses around subexpressions are thus always respected by the  <code>@tensor</code> macro.</p></li><li><p>As explained in the previous subsection, if one respects the  <a href="https://arxiv.org/abs/1402.0939">NCON</a> convention of specifying indices, i.e. positive  integers for the contracted indices and negative indices for the open indices, the  different factors will be reordered and so that the pairwise tensor contractions  contract over indices with smaller integer label first. For example,</p><p><code>@tensor D[:] := A[-1, 3, 1, -2, 2] * B[3, 2, 4, -5] * C[1, 4, -4, -3]</code></p><p>will be evaluated as <code>(A[-1, 3, 1, -2, 2] * C[1, 4, -4, -3]) * B[3, 2, 4, -5]</code>.  Furthermore, in that case the indices of the output tensor (<code>D</code> in this case) do not  need to be specified (using <code>[:]</code> instead), and will be chosen as  <code>(-1, -2, -3, -4, -5)</code>. Note that if two tensors are contracted, all contraction indices  among them will be contracted, even if there are additional contraction indices whose  label is a higher positive number. For example,</p><p><code>@tensor D[:] := A[-1, 3, 2, -2, 1] * B[3, 1, 4, -5] * C[2, 4, -4, -3]</code></p><p>amounts to the original left to right order, because <code>A</code> and <code>B</code> share the first  contraction index <code>1</code>. When <code>A</code> and <code>B</code> are contracted, also the contraction with label  <code>3</code> will be performed, even though contraction index with label <code>2</code> is not yet  &#39;processed&#39;.</p></li><li><p>A specific contraction order can be manually specified by supplying an <code>order</code> keyword  argument to the <code>@tensor</code> macro. The value is a tuple of the contraction indices in the  order that they should be dealt with, e.g. the default order could be changed to first  contract <code>A</code> with <code>C</code> using</p><p><code>@tensor order=(c,b,e,f) begin      D[a, d, j, i, g] := A[a, b, c, d, e] * B[b, e, f, g] * C[c, f, i, j]  end</code></p><p>Here, the same comment as in the NCON style applies; once two tensors are contracted  because they share an index label which is next in the <code>order</code> list, all other indices  with shared label among them will be contracted, irrespective of their order.</p></li></ol><p>In the case of more complex tensor networks, the optimal contraction order cannot always easily be guessed or determined on plain sight. It is then useful to be able to optimise the contraction order automatically, given a model for the complexity of contracting the different tensors in a particular order. This functionality is provided where the cost function being minimised models the computational complexity by counting the number of scalar multiplications. This minimisation problem is solved using the algorithm that was described in <a href="https://journals.aps.org/pre/abstract/10.1103/PhysRevE.90.033315">Physical Review E 90, 033315 (2014)</a>. For a given tensor networ contraction, this algorithm is ran once at compile time, while lowering the tensor espression, and the outcome will be hard coded in the expression resulting from the macro expansion. While the computational complexity of this optimisation algorithm scales itself exponentially in the number of tensors involved in the network, it should still be acceptibly fast (milliseconds up to a few seconds at most) for tensor network contractions with up to around 30 tensors. Information of the optimization process can be obtained during compilation by using the alternative macro <code>@tensoropt_verbose</code>.</p><p>As the cost is determined at compile time, it is not using actual tensor properties (e.g. <code>size(A, i)</code> in the case of arrays) in the cost model, and the cost or extent associated with every index can be specified in various ways, either using integers or floating point numbers or some arbitrary univariate polynomial of an abstract variable, e.g. <code>χ</code>. In the latter case, the optimization assumes the asymptotic limit of large <code>χ</code>.</p><pre><code class="language-julia hljs">@tensoropt D[a, b, c, d] := A[a, e, c, f] * B[g, d, e] * C[g, f, b]
@tensor opt=true D[a, b, c, d] := A[a, e, c, f] * B[g, d, e] * C[g, f, b]
# cost χ for all indices (a, b, c, d, e, f)

@tensoropt (a, b, c, e) D[a, b, c, d] := A[a, e, c, f] * B[g, d, e] * C[g, f, b]
@tensor opt=(a, b, c, e) D[a, b, c, d] := A[a, e, c, f] * B[g, d, e] * C[g, f, b]
# cost χ for indices (a, b, c, e), other indices (d, f) have cost 1

@tensoropt !(a, b, c, e) D[a, b, c, d] := A[a, e, c, f] * B[g, d, e] * C[g, f, b]
@tensor opt=!(a, b, c, e) D[a, b, c, d] := A[a, e, c, f] * B[g, d, e] * C[g, f, b]
# cost 1 for indices (a, b, c, e), other indices (d, f) have cost χ

@tensoropt (a =&gt; χ, b =&gt; χ^2, c =&gt; 2 * χ, e =&gt; 5) begin
    D[a, b, c, d] := A[a, e, c, f] * B[g, d, e] * C[g, f, b]
end
@tensor opt=(a =&gt; χ, b =&gt; χ^2, c =&gt; 2 * χ, e =&gt; 5) begin
    D[a, b, c, d] := A[a, e, c, f] * B[g, d, e] * C[g, f, b]
end
# cost as specified for listed indices, unlisted indices have cost 1 (any symbol for χ can be used)</code></pre><h2 id="Dynamical-tensor-network-contractions-with-ncon-and-@ncon"><a class="docs-heading-anchor" href="#Dynamical-tensor-network-contractions-with-ncon-and-@ncon">Dynamical tensor network contractions with <code>ncon</code> and <code>@ncon</code></a><a id="Dynamical-tensor-network-contractions-with-ncon-and-@ncon-1"></a><a class="docs-heading-anchor-permalink" href="#Dynamical-tensor-network-contractions-with-ncon-and-@ncon" title="Permalink"></a></h2><p>Tensor network practicioners are probably more familiar with the network contractor function <code>ncon</code> to perform a tensor network contraction, as e.g. described in <a href="https://arxiv.org/abs/1402.0939">NCON</a>. In particular, a graphical application <a href="https://www.tensortrace.com">TensorTrace</a> was recently introduced to facilitate the generation of such <code>ncon</code> calls. TensorOperations.jl provides compatibility with this interface by also exposing an <code>ncon</code> function with the same basic syntax</p><pre><code class="language-julia hljs">ncon(list_of_tensor_objects, list_of_index_lists)</code></pre><p>e.g. the example of above is equivalent to</p><pre><code class="language- hljs">@tensor D[:] := A[-1, 3, 1, -2, 2] * B[3, 2, 4, -5] * C[1, 4, -4, -3]
D ≈ ncon((A, B, C), ([-1, 3, 1, -2, 2], [3, 2, 4, -5], [1, 4, -4, -3]))</code></pre><p>where the lists of tensor objects and of index lists can be given as a vector or a tuple. The <code>ncon</code> function necessarily needs to analyze the contraction pattern at runtime, but this can be an advantage, in cases where the contraction is determined by runtime information and thus not known at compile time. A downside from this, besides the fact that this can result in some overhead (though this is typically negligable for anything but very small tensor contractions), is that <code>ncon</code> is type-unstable, i.e. its return type cannot be inferred by the Julia compiler.</p><p>The full call syntax of the <code>ncon</code> method exposed by TensorOperations.jl is</p><pre><code class="language-julia hljs">ncon(tensorlist, indexlist, [conjlist]; order=..., output=...)</code></pre><p>where the first two arguments are those of above. Let us first discuss the keyword arguments. The keyword argument <code>order</code> can be used to change the contraction order, i.e. by specifying which contraction indices need to be processed first, rather than the strictly increasing order <code>[1, 2, ...]</code>, as discussed in the previous subsection. The keyword argument <code>output</code> can be used to specify the order of the output indices, when it is different from the default <code>[-1, -2, ...]</code>.</p><p>The optional positional argument <code>conjlist</code> is a list of <code>Bool</code> variables that indicate whether the corresponding tensor needs to be conjugated in the contraction. So while</p><pre><code class="language-julia hljs">ncon([A, conj(B), C], [[-1, 3, 1, -2, 2], [3, 2, 4, -5], [1, 4, -4, -3]]) ≈
ncon([A, B, C], [[-1, 3, 1, -2, 2], [3, 2, 4, -5], [1, 4, -4, -3]], [false, true, false])</code></pre><p>the latter has the advantage that conjugating <code>B</code> is not an extra step (which creates an additional temporary requiring allocations), but is performed at the same time when it is contracted.</p><p>As an alternative solution to the optional positional arguments, there is also an <code>@ncon</code> macro. It is just a simple wrapper over an <code>ncon</code> call and thus does not analyze the indices at compile time, so that they can be fully dynamical. However, it will transform</p><pre><code class="language-julia hljs">@ncon([A, conj(B), C], indexlist; order=..., output=...)</code></pre><p>into</p><pre><code class="language-julia hljs">ncon(Any[A, B, C], indexlist, [false, true, false]; order=..., output=...)</code></pre><p>so as to get the advantages of just-in-time conjugation (pun intended) using the familiar looking <code>ncon</code> syntax.</p><p>As a proof of principle, let us study the following method for computing the environment to the <code>W</code> isometry in a MERA, as taken from <a href="https://www.tensors.net/mera">Tensors.net</a>, implemented in three different ways:</p><pre><code class="language-julia hljs">function IsoEnvW1(hamAB, hamBA, rhoBA, rhoAB, w, v, u)
    indList1 = Any[[7, 8, -1, 9], [4, 3, -3, 2], [7, 5, 4], [9, 10, -2, 11], [8, 10, 5, 6],
                   [1, 11, 2], [1, 6, 3]]
    indList2 = Any[[1, 2, 3, 4], [10, 7, -3, 6], [-1, 11, 10], [3, 4, -2, 8], [1, 2, 11, 9],
                   [5, 8, 6], [5, 9, 7]]
    indList3 = Any[[5, 7, 3, 1], [10, 9, -3, 8], [-1, 11, 10], [4, 3, -2, 2], [4, 5, 11, 6],
                   [1, 2, 8], [7, 6, 9]]
    indList4 = Any[[3, 7, 2, -1], [5, 6, 4, -3], [2, 1, 4], [3, 1, 5], [7, -2, 6]]
    wEnv = ncon(Any[hamAB, rhoBA, conj(w), u, conj(u), v, conj(v)], indList1) +
           ncon(Any[hamBA, rhoBA, conj(w), u, conj(u), v, conj(v)], indList2) +
           ncon(Any[hamAB, rhoBA, conj(w), u, conj(u), v, conj(v)], indList3) +
           ncon(Any[hamBA, rhoAB, v, conj(v), conj(w)], indList4)
    return wEnv
end

function IsoEnvW2(hamAB, hamBA, rhoBA, rhoAB, w, v, u)
    indList1 = Any[[7, 8, -1, 9], [4, 3, -3, 2], [7, 5, 4], [9, 10, -2, 11], [8, 10, 5, 6],
                   [1, 11, 2], [1, 6, 3]]
    indList2 = Any[[1, 2, 3, 4], [10, 7, -3, 6], [-1, 11, 10], [3, 4, -2, 8], [1, 2, 11, 9],
                   [5, 8, 6], [5, 9, 7]]
    indList3 = Any[[5, 7, 3, 1], [10, 9, -3, 8], [-1, 11, 10], [4, 3, -2, 2], [4, 5, 11, 6],
                   [1, 2, 8], [7, 6, 9]]
    indList4 = Any[[3, 7, 2, -1], [5, 6, 4, -3], [2, 1, 4], [3, 1, 5], [7, -2, 6]]
    wEnv = @ncon(Any[hamAB, rhoBA, conj(w), u, conj(u), v, conj(v)], indList1) +
           @ncon(Any[hamBA, rhoBA, conj(w), u, conj(u), v, conj(v)], indList2) +
           @ncon(Any[hamAB, rhoBA, conj(w), u, conj(u), v, conj(v)], indList3) +
           @ncon(Any[hamBA, rhoAB, v, conj(v), conj(w)], indList4)
    return wEnv
end

function IsoEnvW3(hamAB, hamBA, rhoBA, rhoAB, w, v, u)
    @tensor wEnv[-1, -2, -3] := hamAB[7, 8, -1, 9] * rhoBA[4, 3, -3, 2] * conj(w[7, 5, 4]) *
                                u[9, 10, -2, 11] * conj(u[8, 10, 5, 6]) * v[1, 11, 2] *
                                conj(v[1, 6, 3]) +
                                hamBA[1, 2, 3, 4] * rhoBA[10, 7, -3, 6] * conj(w[-1, 11, 10]) *
                                u[3, 4, -2, 8] * conj(u[1, 2, 11, 9]) * v[5, 8, 6] *
                                conj(v[5, 9, 7]) +
                                hamAB[5, 7, 3, 1] * rhoBA[10, 9, -3, 8] * conj(w[-1, 11, 10]) *
                                u[4, 3, -2, 2] * conj(u[4, 5, 11, 6]) * v[1, 2, 8] *
                                conj(v[7, 6, 9]) +
                                hamBA[3, 7, 2, -1] * rhoAB[5, 6, 4, -3] * v[2, 1, 4] *
                                conj(v[3, 1, 5]) * conj(w[7, -2, 6])
    return wEnv
end</code></pre><p>All indices appearing in this problem are of size <code>χ</code>. For tensors with <code>ComplexF64</code> eltype and values of <code>χ</code> in <code>2:2:32</code>, the reported minimal times using the <code>@belapsed</code> macro from <a href="https://github.com/JuliaCI/BenchmarkTools.jl">BenchmarkTools.jl</a> are given by</p><table><tr><th style="text-align: left">χ</th><th style="text-align: left">IsoEnvW1: ncon</th><th style="text-align: left">IsoEnvW2: @ncon</th><th style="text-align: left">IsoEnvW3: @tensor</th></tr><tr><td style="text-align: left">2</td><td style="text-align: left">0.000154413</td><td style="text-align: left">0.000348091</td><td style="text-align: left">6.4897e-5</td></tr><tr><td style="text-align: left">4</td><td style="text-align: left">0.000208224</td><td style="text-align: left">0.000400065</td><td style="text-align: left">9.5601e-5</td></tr><tr><td style="text-align: left">6</td><td style="text-align: left">0.000558442</td><td style="text-align: left">0.00076453</td><td style="text-align: left">0.000354621</td></tr><tr><td style="text-align: left">8</td><td style="text-align: left">0.00138887</td><td style="text-align: left">0.00150175</td><td style="text-align: left">0.000982109</td></tr><tr><td style="text-align: left">10</td><td style="text-align: left">0.00506386</td><td style="text-align: left">0.00365188</td><td style="text-align: left">0.00288137</td></tr><tr><td style="text-align: left">12</td><td style="text-align: left">0.0126571</td><td style="text-align: left">0.00959403</td><td style="text-align: left">0.00818371</td></tr><tr><td style="text-align: left">14</td><td style="text-align: left">0.0292822</td><td style="text-align: left">0.0216231</td><td style="text-align: left">0.0184712</td></tr><tr><td style="text-align: left">16</td><td style="text-align: left">0.0531353</td><td style="text-align: left">0.0410914</td><td style="text-align: left">0.0359749</td></tr><tr><td style="text-align: left">18</td><td style="text-align: left">0.225333</td><td style="text-align: left">0.0774705</td><td style="text-align: left">0.0688475</td></tr><tr><td style="text-align: left">20</td><td style="text-align: left">0.43358</td><td style="text-align: left">0.139873</td><td style="text-align: left">0.129315</td></tr><tr><td style="text-align: left">22</td><td style="text-align: left">0.601685</td><td style="text-align: left">0.243468</td><td style="text-align: left">0.221995</td></tr><tr><td style="text-align: left">24</td><td style="text-align: left">0.902662</td><td style="text-align: left">0.459746</td><td style="text-align: left">0.427615</td></tr><tr><td style="text-align: left">26</td><td style="text-align: left">1.2379</td><td style="text-align: left">0.66722</td><td style="text-align: left">0.622856</td></tr><tr><td style="text-align: left">28</td><td style="text-align: left">1.84234</td><td style="text-align: left">1.08766</td><td style="text-align: left">1.0322</td></tr><tr><td style="text-align: left">30</td><td style="text-align: left">2.58548</td><td style="text-align: left">1.53826</td><td style="text-align: left">1.44854</td></tr><tr><td style="text-align: left">32</td><td style="text-align: left">3.85758</td><td style="text-align: left">2.44087</td><td style="text-align: left">2.34229</td></tr></table><p>Throughout this range of <code>χ</code> values, method 3 that uses the <code>@tensor</code> macro is consistenly the fastest, both at small <code>χ</code>, where the type stability and the fact that the contraction pattern is analyzed at compile time matters, and at large <code>χ</code>, where the caching of temporaries matters. The direct <code>ncon</code> call has neither of those two features (unless the fourth positional argument is specified, which was not the case here). The <code>@ncon</code> solution provides a hook into the cache and thus is competitive with <code>@tensor</code> for large <code>χ</code>, where the cost is dominated by matrix multiplication and allocations. For small <code>χ</code>, <code>@ncon</code> is also plagued by the runtime analysis of the contraction, but is even worse then <code>ncon</code>. For small <code>χ</code>, the unavoidable type instabilities in <code>ncon</code> implementation seem to make the interaction with the cache hurtful rather than advantageous.</p><h2 id="Index-compatibility-and-checks"><a class="docs-heading-anchor" href="#Index-compatibility-and-checks">Index compatibility and checks</a><a id="Index-compatibility-and-checks-1"></a><a class="docs-heading-anchor-permalink" href="#Index-compatibility-and-checks" title="Permalink"></a></h2><p>Indices with the same label, either open indices on the two sides of the equation, or contracted indices, need to be compatible. For <code>AbstractArray</code> objects, this means they must have the same size. Other tensor types might have more complicated structure associated with their indices, and requires matching between those. The function <a href="../interface/#TensorOperations.checkcontractible"><code>checkcontractible</code></a> is part of the interface that can be used to control when tensors can be contracted with each other along specific indices.</p><p>If indices don&#39;t match, the contraction will spawn an error. However, this can be an error deep within the implementation, at which point the error message will provide little information as to which specific tensors and which indices are producing the mismatch. By adding the keyword argument <code>contractcheck = true</code> to the <code>@tensor</code> macro, explicit checks are enabled that are run before any tensor operation is performed, and when a mismatch is detected, the still have the label information to spawn a more useful error message.</p><p>TODO: continue the following A different type of check is the <code>costcheck</code> keyword argument, which can be given the values <code>:warn</code> or <code>:cache</code>.</p><h2 id="Backends,-multithreading-and-GPUs"><a class="docs-heading-anchor" href="#Backends,-multithreading-and-GPUs">Backends, multithreading and GPUs</a><a id="Backends,-multithreading-and-GPUs-1"></a><a class="docs-heading-anchor-permalink" href="#Backends,-multithreading-and-GPUs" title="Permalink"></a></h2><p>Every index expression will be evaluated as a sequence of elementary tensor operations, i.e. permuted additions, partial traces and contractions, which are implemented for strided arrays as discussed in <a href="../../#Package-features">Package features</a>. In particular, these implementations rely on <a href="https://github.com/Jutho/Strided.jl">Strided.jl</a>, and we refer to this package for a full specification of which arrays are supported. As a rule of thumb, this primarily includes <code>Array</code>s from Julia base, as well as <code>view</code>s thereof if sliced with a combination of <code>Integer</code>s and <code>Range</code>s. Special types such as <code>Adjoint</code> and <code>Transpose</code> from Base are also supported. For permuted addition and partial traces, native Julia implementations are used which could benefit from multithreading if <code>JULIA_NUM_THREADS&gt;1</code>. The binary contraction is performed by first permuting the two input tensors into a form such that the contraction becomes equivalent to one matrix multiplication on the whole data, followed by a final permutation to bring the indices of the output tensor into the desired order. This approach allows to use the highly efficient matrix multiplication kernel (<code>gemm</code>) from BLAS, which is multithreaded by default. There is also a native contraction implementation that is used for e.g. arrays with an <code>eltype</code> that is not <code>&lt;:LinearAlgebra.BlasFloat</code>. It performs the contraction directly without the additional permutations, but still in a cache-friendly and multithreaded way (again relying on <code>JULIA_NUM_THREADS&gt;1</code>). This implementation can also be used for <code>BlasFloat</code> types (but will typically be slower), and the use of BLAS can be controlled by explicitly switching the backend between <code>StridedBLAS</code> and <code>StridedNative</code>.</p><p>The primitive tensor operations are also implemented for <code>CuArray</code> objects of the <a href="https://github.com/JuliaGPU/CUDA.jl">CUDA.jl</a> library. This implementation is essentially a simple wrapper over the cuTENSOR library of NVidia, and will only be loaded when the <code>cuTENSOR</code> library is loaded. The <code>@tensor</code> macro will then automatically work for operations between GPU arrays.</p><p>Mixed operations between host arrays (e.g. <code>Array</code>) and device arrays (e.g. <code>CuArray</code>) will fail however. If one wants to harness the computing power of the GPU to perform all tensor operations, there is a dedicated macro <code>@cutensor</code>. This will transfer all host arrays to the GPU before performing the requested operations. If the output is an existing host array, the result will be copied back. If a new result array is created (i.e. using <code>:=</code>), it will remain on the GPU device and it is up to the user to transfer it back. Arrays are transfered to the GPU just before they are first used, and in a complicated tensor expression, this might have the benefit that transer of the later arrays overlaps with computation of earlier operations.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../">« Home</a><a class="docs-footer-nextpage" href="../functions/">Functions »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Sunday 16 July 2023 16:09">Sunday 16 July 2023</span>. Using Julia version 1.9.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
