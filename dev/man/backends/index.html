<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Backends and Allocators · TensorOperations.jl</title><meta name="title" content="Backends and Allocators · TensorOperations.jl"/><meta property="og:title" content="Backends and Allocators · TensorOperations.jl"/><meta property="twitter:title" content="Backends and Allocators · TensorOperations.jl"/><meta name="description" content="Documentation for TensorOperations.jl."/><meta property="og:description" content="Documentation for TensorOperations.jl."/><meta property="twitter:description" content="Documentation for TensorOperations.jl."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.svg" alt="TensorOperations.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">TensorOperations.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Manual</span><ul><li><a class="tocitem" href="../indexnotation/">Index notation with macros</a></li><li><a class="tocitem" href="../functions/">Functions</a></li><li><a class="tocitem" href="../interface/">Interface</a></li><li class="is-active"><a class="tocitem" href>Backends and Allocators</a><ul class="internal"><li><a class="tocitem" href="#Backends"><span>Backends</span></a></li><li><a class="tocitem" href="#Allocators"><span>Allocators</span></a></li></ul></li><li><a class="tocitem" href="../autodiff/">Automatic differentiation</a></li><li><a class="tocitem" href="../implementation/">Implementation</a></li></ul></li><li><a class="tocitem" href="../../index/">Index</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Manual</a></li><li class="is-active"><a href>Backends and Allocators</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Backends and Allocators</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/Jutho/TensorOperations.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/Jutho/TensorOperations.jl/blob/master/docs/src/man/backends.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Backends-and-Allocators"><a class="docs-heading-anchor" href="#Backends-and-Allocators">Backends and Allocators</a><a id="Backends-and-Allocators-1"></a><a class="docs-heading-anchor-permalink" href="#Backends-and-Allocators" title="Permalink"></a></h1><p>The <code>TensorOperations</code> package is designed to provide powerful tools for performing tensor computations efficiently. In advanced use cases, it can be desirable to squeeze the last drops of performance out of the library, by experimenting with either different micro-optimized implementations of the same operation, or by altering the memory management system. Here, we detail how to access these functionalities. Note that all of the backend and allocator types documented below are not exported, so as not to pollute the name space and since they will typically only be manually configured in expert use cases.</p><h2 id="Backends"><a class="docs-heading-anchor" href="#Backends">Backends</a><a id="Backends-1"></a><a class="docs-heading-anchor-permalink" href="#Backends" title="Permalink"></a></h2><h3 id="Backend-Selection"><a class="docs-heading-anchor" href="#Backend-Selection">Backend Selection</a><a id="Backend-Selection-1"></a><a class="docs-heading-anchor-permalink" href="#Backend-Selection" title="Permalink"></a></h3><p><code>TensorOperations</code> supports multiple backends for tensor contractions, allowing users to choose different implementations based on their specific needs. While special care is taken to ensure good defaults, we also provide the flexibility to select a backend manually. This can be achieved in a variety of ways:</p><ol><li><p><strong>Global setting</strong>: The default backend can be set globally on a per-type basis, as well as a per-function basis. This is achieved by hooking into the implementation of the default backend selection procedure. In particular, this procedure ends up calling <a href="#TensorOperations.select_backend"><code>TensorOperations.select_backend</code></a>`, which can be overloaded to return a different backend.</p></li><li><p><strong>Local setting</strong>: Alternatively, the backend can be set locally for a specific call to either <a href="../indexnotation/#TensorOperations.@tensor"><code>@tensor</code></a>, <a href="../indexnotation/#TensorOperations.ncon"><code>ncon</code></a> or the function-based interface. Both <code>@tensor</code> and <code>ncon</code> accept a keyword argument <code>backend</code>, which will locally override the default backend selection mechanism. The result is that the specified backend will be inserted as a final argument to all calls of the primitive tensor operations. This is also how this can be achieved in the function-based interface.</p></li></ol><pre><code class="language-julia hljs">using TensorOperations
mybackend = TensorOperations.StridedNative()

# inserting a backend into the @tensor macro
@tensor backend = mybackend A[i,j] := B[i,k] * C[k,j]

# inserting a backend into the ncon function
D = ncon([A, B, C], [[1, 2], [2, 3], [3, 1]]; backend=mybackend)

# inserting a backend into the function-based interface
tensoradd(A, pA, conjA, B, pB, conjB, α, β, mybackend)</code></pre><h3 id="Available-Backends"><a class="docs-heading-anchor" href="#Available-Backends">Available Backends</a><a id="Available-Backends-1"></a><a class="docs-heading-anchor-permalink" href="#Available-Backends" title="Permalink"></a></h3><p>All backends that are accepted in the three primitive tensor operations <code>tensoradd!</code>,  <code>tensortrace!</code> and <code>tensorcontract!</code> are subtypes of the abstract type <code>AbstractBackend</code>.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="TensorOperations.AbstractBackend" href="#TensorOperations.AbstractBackend"><code>TensorOperations.AbstractBackend</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">abstract type AbstractBackend</code></pre><p>Abstract supertype for all backends that can be used for tensor operations. In particular, these control different implementations of executing the basic operations.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Jutho/TensorOperations.jl/blob/3433bc982adae18149595da1b795462be8482ee8/src/backends.jl#L3-L8">source</a></section></article><p>TensorOperations.jl provides some options for backends out-of-the box. Firstly, there is the <code>DefaultBackend</code>, which is selected if no backend is specified:</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="TensorOperations.DefaultBackend" href="#TensorOperations.DefaultBackend"><code>TensorOperations.DefaultBackend</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">DefaultBackend()</code></pre><p>Default backend for tensor operations if no explicit backend is specified. This will select an actual implementation backend using the <code>select_backend(tensorfun, tensors...)</code> mechanism.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Jutho/TensorOperations.jl/blob/3433bc982adae18149595da1b795462be8482ee8/src/backends.jl#L11-L16">source</a></section></article><p>The different tensor operations have a general catch-all method in combination with <code>DefaultBackend</code>,  which will then call <code>select_backend</code> to determine teh actual backend to be used, which can  depend on the specific tensor types involved and the operation (<code>tensoradd!</code>, <code>tensortrace!</code>  and <code>tensorcontract!</code>) to be performed.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="TensorOperations.select_backend" href="#TensorOperations.select_backend"><code>TensorOperations.select_backend</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">select_backend([tensorfun::Function], tensors...) -&gt; AbstractBackend</code></pre><p>Select the default backend for the given tensors or tensortypes. If <code>tensorfun</code> is provided, it is possible to more finely control the backend selection based on the function as well.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Jutho/TensorOperations.jl/blob/3433bc982adae18149595da1b795462be8482ee8/src/backends.jl#L26-L31">source</a></section></article><p>Within TensorOperations.jl, the following specific backends are available:</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="TensorOperations.BaseCopy" href="#TensorOperations.BaseCopy"><code>TensorOperations.BaseCopy</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">BaseCopy()</code></pre><p>Backend for tensor operations that should work for all <code>AbstractArray</code> types and only uses functions from the <code>Base</code> module, as well as <code>LinearAlgebra.mul!</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Jutho/TensorOperations.jl/blob/3433bc982adae18149595da1b795462be8482ee8/src/backends.jl#L37-L42">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="TensorOperations.BaseView" href="#TensorOperations.BaseView"><code>TensorOperations.BaseView</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">BaseView()</code></pre><p>Backend for tensor operations that should work for all <code>AbstractArray</code> types and only uses functions from the <code>Base</code> module, as well as <code>LinearAlgebra.mul!</code>, and furthermore tries to avoid any intermediate allocations by using views.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Jutho/TensorOperations.jl/blob/3433bc982adae18149595da1b795462be8482ee8/src/backends.jl#L45-L51">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="TensorOperations.StridedNative" href="#TensorOperations.StridedNative"><code>TensorOperations.StridedNative</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">StridedNative()</code></pre><p>Backend for tensor operations that is based on <code>StridedView</code> objects with native Julia implementations of tensor operations.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Jutho/TensorOperations.jl/blob/3433bc982adae18149595da1b795462be8482ee8/src/backends.jl#L56-L61">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="TensorOperations.StridedBLAS" href="#TensorOperations.StridedBLAS"><code>TensorOperations.StridedBLAS</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">StridedBLAS()</code></pre><p>Backend for tensor operations that is based on using <code>StridedView</code> objects and rephrasing the tensor operations as BLAS operations.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Jutho/TensorOperations.jl/blob/3433bc982adae18149595da1b795462be8482ee8/src/backends.jl#L64-L69">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="TensorOperations.cuTENSORBackend" href="#TensorOperations.cuTENSORBackend"><code>TensorOperations.cuTENSORBackend</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">cuTENSORBackend()</code></pre><p>Backend for tensor operations that is based on the NVIDIA cuTENSOR library.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Jutho/TensorOperations.jl/blob/3433bc982adae18149595da1b795462be8482ee8/src/backends.jl#L76-L79">source</a></section></article><p>Here, arrays that are strided are typically handled most efficiently by the <code>Strided.jl</code>-based backends. By default, the <code>StridedBLAS</code> backend is used for element types that support BLAS operations, as it seems that the performance gains from using BLAS outweigh the overhead of sometimes having to allocate intermediate permuted arrays.</p><p>On the other hand, the <code>BaseCopy</code> and <code>BaseView</code> backends are used for arrays that are not strided. These are designed to be as general as possible, and as a result are not as performant as specific implementations. Nevertheless, they can be useful for debugging purposes or for working with custom tensor types that have limited support for methods outside of <code>Base</code>.</p><p>Finally, we also provide a <code>cuTENSORBackend</code> for use with the <code>cuTENSOR.jl</code> library, which is a NVidia GPU-accelerated tensor contraction library. This backend is only available through a package extension for <code>cuTENSOR</code>.</p><p>Finally, there is also the following self-explanatory backend:</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="TensorOperations.NoBackend" href="#TensorOperations.NoBackend"><code>TensorOperations.NoBackend</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">NoBackend()</code></pre><p>Backend that will be returned if no suitable backend can be found for the given tensors.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Jutho/TensorOperations.jl/blob/3433bc982adae18149595da1b795462be8482ee8/src/backends.jl#L19-L23">source</a></section></article><h3 id="Custom-Backends"><a class="docs-heading-anchor" href="#Custom-Backends">Custom Backends</a><a id="Custom-Backends-1"></a><a class="docs-heading-anchor-permalink" href="#Custom-Backends" title="Permalink"></a></h3><p>Users can also define their own backends, to facilitate experimentation with new implementations. This can be done by defining a new type that is a subtype of <code>AbstractBackend</code>, and dispatching on this type in the implementation of the primitive tensor operations. In particular, the only required implemented methods are <a href="../functions/#TensorOperations.tensoradd!"><code>tensoradd!</code></a>, <a href="../functions/#TensorOperations.tensortrace!"><code>tensortrace!</code></a>, <a href="../functions/#TensorOperations.tensorcontract!"><code>tensorcontract!</code></a>.</p><p>For example, <a href="https://github.com/lkdvos/TensorOperationsTBLIS.jl"><code>TensorOperationsTBLIS</code></a> is a wrapper that provides a backend for tensor contractions using the <a href="https://github.com/devinamatthews/tblis">TBLIS</a> library.</p><h2 id="Allocators"><a class="docs-heading-anchor" href="#Allocators">Allocators</a><a id="Allocators-1"></a><a class="docs-heading-anchor-permalink" href="#Allocators" title="Permalink"></a></h2><p>Evaluating complex tensor networks is typically done most efficiently by pairwise operations. As a result, this procedure often requires the allocation of many temporary arrays, which can affect performance for certain operations. To mitigate this, <code>TensorOperations</code> exposes an allocator system, which allows users to more finely control the allocation of both output tensors and temporary tensors.</p><p>In particular, the allocator system is used in multiple ways: As mentioned before, it can be used to allocate and free the intermediate tensors that are required to evaluate a tensor network in a pairwise fashion. Additionally, it can also be used to allocate and free temporary objects that arise when reshaping and permuting input tensors, for example when making them compatible with BLAS instructions.</p><h3 id="Allocator-Selection"><a class="docs-heading-anchor" href="#Allocator-Selection">Allocator Selection</a><a id="Allocator-Selection-1"></a><a class="docs-heading-anchor-permalink" href="#Allocator-Selection" title="Permalink"></a></h3><p>The allocator system can only be accessed <em>locally</em>, by passing an allocator to the <code>@tensor</code> macro, the <code>ncon</code> function, or the function-based interface.</p><pre><code class="language-julia hljs">using TensorOperations
myallocator = TensorOperations.ManualAllocator()

# inserting a backend into the @tensor macro
@tensor allocator = myallocator A[i,j] := B[i,k] * C[k,j]

# inserting an allocator into the ncon function
D = ncon([A, B, C], [[1, 2], [2, 3], [3, 1]]; allocator=myallocator)

# inserting a backend into the function-based interface
tensoradd(A, pA, conjA, B, pB, conjB, α, β, DefaultBackend(), myallocator)</code></pre><p>Important to note here is that the backend system is prioritized over the allocator system. In particular, this means that the backend will be selected <strong>first</strong>, while only then the allocator should be inserted.</p><h3 id="Available-Allocators"><a class="docs-heading-anchor" href="#Available-Allocators">Available Allocators</a><a id="Available-Allocators-1"></a><a class="docs-heading-anchor-permalink" href="#Available-Allocators" title="Permalink"></a></h3><p><code>TensorOperations</code> also provides some options for allocators out-of-the box.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="TensorOperations.DefaultAllocator" href="#TensorOperations.DefaultAllocator"><code>TensorOperations.DefaultAllocator</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">DefaultAllocator()</code></pre><p>Default allocator for tensor operations if no explicit allocator is specified. This will just use the standard constructor for the tensor type, and thus probably uses Julia&#39;s default memory manager.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Jutho/TensorOperations.jl/blob/3433bc982adae18149595da1b795462be8482ee8/src/implementation/allocator.jl#L4-L10">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="TensorOperations.ManualAllocator" href="#TensorOperations.ManualAllocator"><code>TensorOperations.ManualAllocator</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">ManualAllocator()</code></pre><p>Allocator that bypasses Julia&#39;s memory management for temporary tensors by leveraging <code>Libc.malloc</code> and <code>Libc.free</code> directly. This can be useful for reducing the pressure on the garbage collector. This backend will allocate using <code>DefaultAllocator</code> for output tensors that escape the <code>@tensor</code> block, which will thus still be managed using Julia&#39;s GC. The other tensors will be backed by <code>PtrArray</code> instances, from <code>PtrArrays.jl</code>, thus requiring compatibility with that interface.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Jutho/TensorOperations.jl/blob/3433bc982adae18149595da1b795462be8482ee8/src/implementation/allocator.jl#L33-L41">source</a></section></article><p>By default, the <code>DefaultAllocator</code> is used, which uses Julia&#39;s built-in memory management system. Optionally, it can be useful to use the <code>ManualAllocator</code>, as the manual memory management reduces the pressure on the garbage collector. In particular in multi-threaded applications, this can sometimes lead to a significant performance improvement.</p><p>Finally, users can also opt to use the <code>Bumper.jl</code> system, which pre-allocates a slab of memory that can be re-used afterwards. This is available through a package extension for <code>Bumper</code>. Here, the <code>allocator</code> object is just the provided buffers, which are then used to store the intermediate tensors.</p><pre><code class="language-julia hljs">using TensorOperations, Bumper
buf = Bumper.default_buffer()
@no_escape buf begin
    @tensor allocator = buf A[i,j] := B[i,k] * C[k,j]
end</code></pre><p>For convenience, the construction above is also provided in a specialized macro form which is fully equivalent:</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="TensorOperations.@butensor" href="#TensorOperations.@butensor"><code>TensorOperations.@butensor</code></a> — <span class="docstring-category">Macro</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">@butensor tensor_expr</code></pre><p>Use Bumper.jl to handle allocation of temporary tensors. This macro will use the default buffer and automatically reset it after the tensor expression has been evaluated. This macro is equivalent to <code>@no_escape @tensor tensor_expr</code> with all temporary allocations handled by Bumper.jl.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>This macro requires Bumper.jl to be installed and loaded. This can be achieved by running <code>using Bumper</code> or <code>import Bumper</code> before using the macro.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Jutho/TensorOperations.jl/blob/3433bc982adae18149595da1b795462be8482ee8/src/indexnotation/tensormacros.jl#L315-L326">source</a></section></article><p>When using the <code>cuTENSORBackend()</code> and no allocator is specified, it will automatically select the allocator <code>CUDAAllocator()</code>, which will create new temporaries as <code>CuArray</code> objects. However, <code>CUDAAllocator</code> has three type parameters which can be used to customize the behavior of the allocator with respect to temporaries, as well as input and output tensors.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="TensorOperations.CUDAAllocator" href="#TensorOperations.CUDAAllocator"><code>TensorOperations.CUDAAllocator</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">CUDAAllocator{Mout,Min,Mtemp}()</code></pre><p>Allocator that uses the CUDA memory manager and will thus allocate <code>CuArray</code> instances. The parameters <code>Min</code>, <code>Mout</code>, <code>Mtemp</code> can be any of the CUDA.jl memory types, i.e.  <code>CUDA.DeviceMemory</code>, <code>CUDA.UnifiedMemory</code> or <code>CUDA.HostMemory</code>.</p><ul><li><code>Mout</code> is used to determine how to deal with output tensors; with <code>Mout=CUDA.HostMemory</code> or <code>Mout=CUDA.UnifiedMemory</code> the CUDA runtime will ensure that the data is also available at in the host memory, and can thus be converted back to normal arrays using <code>unsafe_wrap(Array, outputtensor)</code>. If <code>Mout=CUDA.DeviceMemory</code> the data will remain on the GPU, untill an explict <code>Array(outputtensor)</code> is called.</li><li><code>Min</code> is used to determine how to deal with input tensors; with <code>Min=CUDA.HostMemory</code> the CUDA runtime will itself take care of transferring the data to the GPU, otherwise it is copied explicitly.</li><li><code>Mtemp</code> is used to allocate space for temporary tensors; it defaults to <code>CUDA.default_memory</code> which is <code>CUDA.DeviceMemory</code>. Only if many or huge temporary tensors are expected could it be useful to choose <code>CUDA.UnifiedMemory</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Jutho/TensorOperations.jl/blob/3433bc982adae18149595da1b795462be8482ee8/src/implementation/allocator.jl#L13-L30">source</a></section></article><h3 id="Custom-Allocators"><a class="docs-heading-anchor" href="#Custom-Allocators">Custom Allocators</a><a id="Custom-Allocators-1"></a><a class="docs-heading-anchor-permalink" href="#Custom-Allocators" title="Permalink"></a></h3><p>Users can also define their own allocators, to facilitate experimentation with new implementations. Here, no restriction is made on the type of the allocator, and any object can be passed as an allocator. The required implemented methods are <a href="../interface/#TensorOperations.tensoralloc"><code>tensoralloc</code></a> and <a href="../interface/#TensorOperations.tensorfree!"><code>tensorfree!</code></a>.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../interface/">« Interface</a><a class="docs-footer-nextpage" href="../autodiff/">Automatic differentiation »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.7.0 on <span class="colophon-date" title="Monday 14 October 2024 11:22">Monday 14 October 2024</span>. Using Julia version 1.11.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
